{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiment on Inner Speech dataset\n",
    "\n",
    "Run this experiment to obtain a similar result as the one publisher in the paper\n",
    "\n",
    "```latex\n",
    "@article{gallo2024eeg,\n",
    "  title={Thinking is Like a Sequence of Words},\n",
    "  author={Gallo, Ignzio and Coarsh, Silvia},\n",
    "  journal={IJCNN},\n",
    "  volume={??},\n",
    "  pages={??--??},\n",
    "  year={2024},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "```\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import yaml\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_save_dir(args):\n",
    "    if False: #\"subject_num\" in args:\n",
    "        if type(args['subject_num']) is not list:\n",
    "            args['subject_num'] = [args['subject_num']]\n",
    "\n",
    "        subjs_str = ','.join(str(x) for x in args['subject_num'])\n",
    "        args['save_dir'] = os.path.join(args['save_dir'], subjs_str, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    else:\n",
    "        args['save_dir'] = os.path.join(args['save_dir'], datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    if not os.path.isdir(args['save_dir']):\n",
    "        os.makedirs(args['save_dir'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, column_names):\n",
    "        column_names.insert(0, \"time_stamp\")\n",
    "        self.df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    def add_row(self, row_list):\n",
    "        row_list.insert(0, str(dt.datetime.now()))\n",
    "        # print(row_list)\n",
    "        self.df.loc[len(self.df)] = row_list\n",
    "\n",
    "    def save_to_csv(self, filepath):\n",
    "        self.df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_name(sub_id):\n",
    "    # Standarize subjects name\n",
    "    return f\"sub-{sub_id:02}\"\n",
    "\n",
    "def ses_name(ses_id):\n",
    "    # Standarize session name\n",
    "    return f\"ses-{ses_id:02}\"\n",
    "\n",
    "\"\"\"\n",
    "Events data. Each event data file (in .dat format) contains a four column matrix where each row corresponds\n",
    "to one trial. The first two columns were obtained from the raw events, by deleting the trigger column (second\n",
    "column of the raw events) and renumbering the classes 31, 32, 33, 34 as 0, 1, 2, 3, respectively. Finally, the last\n",
    "two columns correspond to condition and session number, respectively. Thus, the resulting final structure of the\n",
    "events data file is as depicted in Table 5.\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "Sample                                     | Trial’s class         | Trials’ condition        | Trials’ session\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "Sample at which the event occurred         | 0 = “Arriba” (up)     | 0 = Pronounced speech    | 1 = session 1\n",
    "(Numbered starting at n = 0, corresponding | 1 = “Abajo” (down)    | 1 = Inner speech         | 2 = session 2\n",
    "to the beginning of the recording)         | 2 = “Derecha” (right) | 2 = Visualized condition | 3 = session 3\n",
    "                                           | 3 = “Izquierda” (left)|                          |\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def load_events(root_dir, subject_id, N_B):\n",
    "    subject_str = sub_name(subject_id)\n",
    "    session_str = ses_name(N_B)\n",
    "    # Create file Name\n",
    "    # file_name =root_dir+\"/derivatives/\"+subject_str+\"/ses-0\"+str(N_B)+\"/\"+subject_str+\"_ses-0\"+str(N_B)+\"_events.dat\"\n",
    "    file_name = os.path.join(root_dir, subject_str, session_str, subject_str+\"_\"+session_str+\"_events.dat\")\n",
    "    # Load Events\n",
    "    events = np.load(file_name,allow_pickle=True)\n",
    "    \n",
    "    return events    \n",
    "\n",
    "\n",
    "def select_time_window_single(X, t_start=1, t_end=2.5, fs=256):\n",
    "    s_max=X.shape[0]\n",
    "    start = max(round(t_start * fs), 0)\n",
    "    end = min(round(t_end * fs), s_max)\n",
    "\n",
    "    #Copy interval\n",
    "    X = X[start:end, :]\n",
    "    return X\n",
    "\n",
    "\n",
    "# Code from: https://github.com/N-Nieto/Inner_Speech_Dataset\n",
    "def extract_data_from_subject(root_dir, subject_id, datatype):\n",
    "    \"\"\"\n",
    "    Load all blocks for one subject and stack the results in X\n",
    "    Reading from 'derivatives' directory: folder, containing five files obtained after the proposed processing: \n",
    "    EEG data, Baseline data, External electrodes data, Events data and a Report file. \n",
    "    For more details about processing see page 5 of https://www.nature.com/articles/s41597-022-01147-2 PDF\n",
    "    \"\"\"\n",
    "    data=dict()\n",
    "    y=dict()\n",
    "    session_id_arr=[1,2,3]\n",
    "    datatype=datatype.lower()\n",
    "    \n",
    "    for session_id in session_id_arr:\n",
    "        # name correction if subject_idubj is less than 10\n",
    "        subject_str = sub_name(subject_id)   \n",
    "        session_str = ses_name(session_id)\n",
    "            \n",
    "        y[session_id] = load_events(root_dir, subject_id, session_id)\n",
    "        \n",
    "        # three consecutive sessions for each partecipant: baseline, inner speech, visualized condition\n",
    "        if datatype==\"eeg\": # 128 active EEG channels\n",
    "            #  load data and events\n",
    "            file_name = os.path.join(root_dir, subject_str, session_str, subject_str+'_'+session_str+'_eeg-epo.fif')\n",
    "            X= mne.read_epochs(file_name,verbose='WARNING')\n",
    "            data[session_id]= X._data\n",
    "            \n",
    "        elif datatype==\"exg\": # 8 External electrodes\n",
    "            file_name = os.path.join(root_dir, subject_str, session_str, subject_str+'_'+session_str+'_exg-epo.fif')\n",
    "            X= mne.read_epochs(file_name,verbose='WARNING')\n",
    "            data[session_id]= X._data\n",
    "        \n",
    "        elif datatype==\"baseline\":\n",
    "            file_name = os.path.join(root_dir, subject_str, session_str, subject_str+'_'+session_str+'_baseline-epo.fif')\n",
    "            X= mne.read_epochs(file_name,verbose='WARNING')\n",
    "            data[session_id]= X._data\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Invalid Datatype\")\n",
    "         \n",
    "    X = np.vstack((data.get(1),data.get(2),data.get(3))) \n",
    "    Y = np.vstack((y.get(1),y.get(2),y.get(3))) \n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, Y, sampling_rate, task, transform=None, t_start=1.5, t_end=3.5, t_win=1.0): \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.transform = transform\n",
    "        # Select the useful par of each trial. Time in seconds\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end  \n",
    "        self.t_win = t_win\n",
    "        self.task = task\n",
    "        # sliding window on the source timeseries\n",
    "        self.data, self.targets = X, Y # sliding_window(X, Y, \n",
    "        #    self.t_start, self.t_end, self.sampling_rate, t_stride=0.05, t_padding=0.05)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        # Cut usefull time. i.e action interval plus a random value\n",
    "        # s = random.random() - 0.5 # aug1: select a 2 sec. random win range [1.0, 4.0]\n",
    "        # x = select_time_window_single(X=x, t_start=self.t_start+s, t_end=self.t_end+s, fs=self.sampling_rate)\n",
    "\n",
    "        if self.task == 'train':\n",
    "            s = random.uniform(self.t_start, self.t_end-self.t_win) # aug2: select a t_win sec. random win range [t_start, t_end-t_win]\n",
    "            # print(f\"selected win: [{s}s,{s+1}s] -> [{int(s*self.sampling_rate)},{int((s+1)*self.sampling_rate)}], max: [0,{x.shape[0]-1}]\")\n",
    "            x = select_time_window_single(X=x, t_start=s, t_end=s+self.t_win, fs=self.sampling_rate)\n",
    "        else:\n",
    "            x = select_time_window_single(X=x, t_start=self.t_start+self.t_win, t_end=self.t_start+2*self.t_win, fs=self.sampling_rate)\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_innerspeech_raw(args):\n",
    "    # Data Type\n",
    "    datatype = \"EEG\"\n",
    "\n",
    "    # Sampling rate\n",
    "    fs = args['sampling_rate'] # 1024 # 256\n",
    "\n",
    "    # Select the useful par of each trial. Time in seconds\n",
    "    t_start = 1.5\n",
    "    t_end = 3.5   \n",
    "\n",
    "    X = np.array([])\n",
    "    Y = np.array([])\n",
    "    for subj in args['subject_num']:\n",
    "        # Load all trials for each subject\n",
    "        xs, ys = extract_data_from_subject(args['data_dir'], subj, datatype)\n",
    "        X = np.concatenate([X, xs]) if X.size else xs\n",
    "        Y = np.concatenate([Y, ys]) if Y.size else ys\n",
    "\n",
    "    # Select Trials’ condition 1 = Inner speech \n",
    "    Y_inner=np.where(Y[:,2]==1)\n",
    "    Y1=Y[Y_inner]\n",
    "    X1=X[Y_inner]\n",
    "    Y1 = Y1[:,1] # get class info only\n",
    "    X1 = np.transpose(X1, (0, 2, 1)) # transform into (batch, vocab_size, emb_dim)\n",
    "\n",
    "    SCALE = 1000\n",
    "    X1=X1*SCALE\n",
    "\n",
    "    # Random split using fixed random_state\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.float32(X1), Y1.astype(int), test_size=0.2, random_state=1)\n",
    "    print(\"Training orig. shape:\", X_train.shape)\n",
    "    print(\"Test orig. shape:\", X_test.shape)\n",
    "\n",
    "    # Convert data to DataLoader\n",
    "    train_dataset = InnerSpeechDataset(X_train, Y_train, fs, task='train', t_start=1.5, t_end=3.5, t_win=1.0)\n",
    "    print(\"Input pattern shape:\", train_dataset.__getitem__(0)[0].shape)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "    test_dataset = InnerSpeechDataset(X_test, Y_test, fs, task='test', t_start=1.5, t_end=3.5, t_win=1.0) \n",
    "    test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Deep neural network based on a basic Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetTraST(nn.Module):\n",
    "    def __init__(self, args): \n",
    "        super(NetTraST, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(args['vocab_size'])\n",
    "        p = args['kernel_size'] // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels=args['vocab_size'], out_channels=args['kernel_num'], kernel_size=args['kernel_size'], stride=1, padding=p)\n",
    "        \n",
    "        #self.conv2 = nn.Conv1d(in_channels=args['embed_dim'], out_channels=args['kernel_num'], kernel_size=args['kernel_size'], stride=1, padding=p)\n",
    "        self.upsamp = nn.Upsample((args['embed_dim']))\n",
    "        \n",
    "        self.rrelu = nn.RReLU(0.1, 0.3)\n",
    "        nl=3 \n",
    "        self.spatial_tra = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args['embed_dim'],\n",
    "                nhead=args['nhead'],\n",
    "                dim_feedforward=args['dim_feedforward'],\n",
    "            ),\n",
    "            num_layers=nl,\n",
    "        )\n",
    "        #self.temporal_tra = nn.TransformerEncoder(\n",
    "        #    nn.TransformerEncoderLayer(\n",
    "        #        d_model=args['vocab_size'],\n",
    "        #        nhead=args['nhead'],\n",
    "        #        dim_feedforward=args['dim_feedforward'],\n",
    "        #    ),\n",
    "        #    num_layers=nl,\n",
    "        #)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args['kernel_num'],\n",
    "                nhead=args['nhead'],\n",
    "                dim_feedforward=args['dim_feedforward'],\n",
    "            ),\n",
    "            num_layers=args['num_layers'],\n",
    "        )\n",
    "        self.batch_norm3 = nn.BatchNorm1d(args['kernel_num'])\n",
    "        self.fl = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(args['kernel_num']*args['embed_dim'], args['kernel_num'])\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.fc2 = nn.Linear(args['kernel_num'], args['class_num'])\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x1 = self.conv1(x) \n",
    "        x1 = self.spatial_tra(x1)\n",
    "        \n",
    "        #x2 = x.permute(0, 2, 1)\n",
    "        #x2 = self.conv2(x2) \n",
    "        #x2 = self.temporal_tra(x2)\n",
    "        #x2 = self.upsamp(x2)\n",
    "\n",
    "        x = x1 #x1+x2 \n",
    "        #x = torch.cat((x1, x2), 1)\n",
    "        \n",
    "        # Reshape the input for the Transformer layer\n",
    "        x = x.permute(2, 0, 1)  # Change the shape to (sequence_length, batch_size, input_size)\n",
    "        x = self.transformer(x)\n",
    "        # Reshape the output back to the original shape\n",
    "        x = x.permute(1, 2, 0)  # Change the shape to (batch_size, input_size, sequence_length)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.fl(x)\n",
    "        x = self.rrelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation_raw(args, model, test_loader, criterion):\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0\n",
    "        test_corrects = torch.tensor(0, device=args['device'])\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            corrects = (torch.max(outputs, 1)[1].view(labels.size()).data == labels.data).sum()\n",
    "            test_corrects += corrects\n",
    "            tot_loss += loss\n",
    "\n",
    "        ts_acc = 100.0 * test_corrects/len(test_loader.dataset)\n",
    "        #print(f'Test Accuracy: {ts_acc:.4f}, Test loss: {tot_loss:.6f}')  \n",
    "    return ts_acc.cpu().item(), tot_loss\n",
    "\n",
    "\n",
    "def train_raw(args, model, train_loader, optimizer, criterion, test_loader, metrics, subj, scheduler=None): \n",
    "    #metrics = Metrics([\"epoch\", \"lr\", \"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\", \"best_test_acc\"])\n",
    "    # Training loop\n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    steps = 0\n",
    "    loop_obj = tqdm(range(args['epochs']))\n",
    "    loop_obj.set_postfix_str(f\"Best val. acc.: {best_acc:.4f}\")  # Adds text after progressbar\n",
    "    for epoch in loop_obj:\n",
    "        loop_obj.set_description(f\"Subj.: {subj}, Training epoch: {epoch+1}\")  # Adds text before progessbar\n",
    "        train_corrects = torch.tensor(0, device=args['device'])\n",
    "        tot_loss = 0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(args['device'])\n",
    "\n",
    "            labels = labels.to(args['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            corrects = (torch.max(outputs, 1)[1].view(labels.size()).data == labels.data).sum()\n",
    "            train_corrects += corrects\n",
    "            tot_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler: scheduler.step()\n",
    "\n",
    "        tr_acc = 100.0 * train_corrects/len(train_loader.dataset)\n",
    "        # Validation\n",
    "        dev_acc, test_loss = evaluation_raw(args, model, test_loader, criterion)\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            patience_counter = 0\n",
    "            #torch.save(model, os.path.join(args['save_dir'], f\"{model.__class__.__name__}_model_best.pt\"))\n",
    "            loop_obj.set_postfix_str(f\"Best val. acc.: {best_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        #EP=args['epochs']\n",
    "        #print(f'Epoch [{epoch+1}/{EP}] Tr. Loss: {tot_loss.item():.4f} Val. Accuracy: {dev_acc:.4f} Best Val. Accuracy: {best_acc:.4f}')\n",
    "        lr=optimizer.param_groups[0][\"lr\"]\n",
    "        metrics.add_row([epoch+1, lr, tot_loss.cpu().item(), tr_acc.cpu().item(), test_loss.cpu().item(), dev_acc, best_acc])\n",
    "        metrics.save_to_csv(os.path.join(args['save_dir'], \"metrics_classifciation.csv\"))\n",
    "\n",
    "        if patience_counter > args['early_stopping_patience']:\n",
    "            print(f\"Early stopping... {patience_counter} > {args['early_stopping_patience']}\")\n",
    "            break\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment\n",
    "\n",
    "- change the parameter '*subject_num*' in the *args* dictionary to change the subject to one of the following \n",
    "  - 'subject_num': [1]\n",
    "  - 'subject_num': [2]\n",
    "  - ...\n",
    "  - 'subject_num': [10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_args():\n",
    "    args = {\n",
    "        'class_num': 4,\n",
    "        'dropout': 0.1 ,\n",
    "        'nhead': 4 ,\n",
    "        'dim_feedforward': 256 ,\n",
    "        'num_layers': 5 ,\n",
    "        'embed_dim': 128,\n",
    "        'vocab_size': 1024,\n",
    "        'kernel_num': 128,\n",
    "        'kernel_size': 3, \n",
    "        'batch_size': 128 ,\n",
    "        'epochs': 1000 ,\n",
    "        'early_stopping_patience': 300 ,\n",
    "        'lr': 0.001 ,\n",
    "        'log_interval': 1,\n",
    "        'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "        'data_dir': \"/home/jovyan/nfs/igallo/datasets/EEG/Inner_Speech_Dataset/derivatives_no_filter/\", \n",
    "        'save_dir': 'experiments/transformer/inner_speech/no_filter_swin1sec/',\n",
    "        'save_best': True,\n",
    "        'verbose': True,\n",
    "        'test_interval': 100,\n",
    "        'save_interval': 500,\n",
    "        'sampling_rate': 1024,\n",
    "        'subject_num': [2] # [1,2,3,4,5,6,7,8,9,10], #  \n",
    "    }\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation 2\n",
    "- NO Conv2\n",
    "- NO Temporal transformer T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_run():\n",
    "    args = get_default_args()\n",
    "    create_save_dir(args)\n",
    "    with open(os.path.join(args['save_dir'], \"config.yaml\"), \"w\") as f:\n",
    "        yaml.dump(\n",
    "            args, stream=f, default_flow_style=False, sort_keys=False\n",
    "        )\n",
    "\n",
    "    # For all the results\n",
    "    metrics = Metrics([\"epoch\", \"lr\", \"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\", \"best_test_acc\"])\n",
    "    results = {}\n",
    "    for sub in range(1,11):\n",
    "        args['subject_num'] = [sub]\n",
    "        model = NetTraST(args)\n",
    "        model = model.to(args['device'])\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters()) #, lr=args['lr'])\n",
    "\n",
    "        train_loader, test_loader = read_innerspeech_raw(args)\n",
    "        print(\"Training size:\", len(train_loader.dataset))\n",
    "        print(\"Test size:\", len(test_loader.dataset))\n",
    "\n",
    "        best_acc = train_raw(args, model, train_loader, optimizer, criterion, test_loader, metrics, sub) \n",
    "        #torch.save(model, os.path.join(args['save_dir'], f\"{model.__class__.__name__}_model_last.pt\"))\n",
    "        results[sub] = best_acc\n",
    "        accs = np.array(list(results.values()))\n",
    "        print(f'acc: mean={np.mean(accs):.4f}%, std={np.std(accs):.4f}%')\n",
    "\n",
    "    # Print subject results\n",
    "    str = f\"RESULTS FOR 10 SUBJECTS\\n\"\n",
    "    str += '--------------------------------\\n'\n",
    "    for key, value in results.items():\n",
    "        str += f'Subject {key}: {value:.4f} %\\n'\n",
    "    accs = np.array(list(results.values()))\n",
    "    str += f'mean: {np.mean(accs):.4f}%, std: {np.std(accs):.4f}%\\n'\n",
    "    print(str)\n",
    "    with open(os.path.join(args['save_dir'], \"mean_results.txt\"), \"w\") as f:\n",
    "        f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (160, 4609, 128)\n",
      "Test orig. shape: (40, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 160\n",
      "Test size: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 1, Training epoch: 308:  31%|███       | 307/1000 [01:46<04:00,  2.88it/s, Best val. acc.: 35.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=35.0000%, std=0.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 2, Training epoch: 307:  31%|███       | 306/1000 [01:56<04:23,  2.63it/s, Best val. acc.: 47.9167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=41.4583%, std=6.4583%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (176, 4609, 128)\n",
      "Test orig. shape: (44, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 176\n",
      "Test size: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 3, Training epoch: 606:  60%|██████    | 605/1000 [03:33<02:19,  2.84it/s, Best val. acc.: 40.9091]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=41.2753%, std=5.2796%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 4, Training epoch: 303:  30%|███       | 302/1000 [01:58<04:33,  2.56it/s, Best val. acc.: 37.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=40.3314%, std=4.8557%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 5, Training epoch: 309:  31%|███       | 308/1000 [02:02<04:36,  2.51it/s, Best val. acc.: 41.6667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=40.5985%, std=4.3758%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (172, 4609, 128)\n",
      "Test orig. shape: (44, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 172\n",
      "Test size: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 6, Training epoch: 514:  51%|█████▏    | 513/1000 [03:20<03:10,  2.56it/s, Best val. acc.: 43.1818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=41.0290%, std=4.1089%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 7, Training epoch: 453:  45%|████▌     | 452/1000 [05:16<06:23,  1.43it/s, Best val. acc.: 33.3333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=39.9297%, std=4.6608%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (160, 4609, 128)\n",
      "Test orig. shape: (40, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 160\n",
      "Test size: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 8, Training epoch: 350:  35%|███▍      | 349/1000 [04:28<08:20,  1.30it/s, Best val. acc.: 37.5000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=39.6259%, std=4.4332%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 9, Training epoch: 496:  50%|████▉     | 495/1000 [08:01<08:10,  1.03it/s, Best val. acc.: 47.9167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=40.5471%, std=4.9253%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training orig. shape: (192, 4609, 128)\n",
      "Test orig. shape: (48, 4609, 128)\n",
      "Input pattern shape: torch.Size([1024, 128])\n",
      "Training size: 192\n",
      "Test size: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 10, Training epoch: 323:  32%|███▏      | 322/1000 [05:16<11:06,  1.02it/s, Best val. acc.: 33.3333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping... 301 > 300\n",
      "acc: mean=39.8258%, std=5.1494%\n",
      "RESULTS FOR 10 SUBJECTS\n",
      "--------------------------------\n",
      "Subject 1: 35.0000 %\n",
      "Subject 2: 47.9167 %\n",
      "Subject 3: 40.9091 %\n",
      "Subject 4: 37.5000 %\n",
      "Subject 5: 41.6667 %\n",
      "Subject 6: 43.1818 %\n",
      "Subject 7: 33.3333 %\n",
      "Subject 8: 37.5000 %\n",
      "Subject 9: 47.9167 %\n",
      "Subject 10: 33.3333 %\n",
      "mean: 39.8258%, std: 5.1494%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "single_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_clip_env",
   "language": "python",
   "name": "open_clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
