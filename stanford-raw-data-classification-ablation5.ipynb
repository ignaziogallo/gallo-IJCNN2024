{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiment on Stanforf EEG Imagined speech dataset\n",
    "\n",
    "Run this experiment to obtain a similar result as the one publisher in the paper\n",
    "\n",
    "```latex\n",
    "@article{gallo2024eeg,\n",
    "  title={Thinking is Like a Sequence of Words},\n",
    "  author={Gallo, Ignzio and Coarsh, Silvia},\n",
    "  journal={IJCNN},\n",
    "  volume={??},\n",
    "  pages={??--??},\n",
    "  year={2024},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "```\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset\n",
    "import yaml\n",
    "import mne\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import scipy.io\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_save_dir(args):\n",
    "    if \"subject_num\" in args:\n",
    "        if type(args['subject_num']) is not list:\n",
    "            args['subject_num'] = [args['subject_num']]\n",
    "\n",
    "        subjs_str = ','.join(str(x) for x in args['subject_num'])\n",
    "        args['save_dir'] = os.path.join(args['save_dir'], subjs_str, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    else:\n",
    "        args['save_dir'] = os.path.join(args['save_dir'], datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    if not os.path.isdir(args['save_dir']):\n",
    "        os.makedirs(args['save_dir'])\n",
    "    print(\"Saving results in:\", args['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, column_names):\n",
    "        column_names.insert(0, \"time_stamp\")\n",
    "        self.df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    def add_row(self, row_list):\n",
    "        row_list.insert(0, str(dt.datetime.now()))\n",
    "        # print(row_list)\n",
    "        self.df.loc[len(self.df)] = row_list\n",
    "\n",
    "    def save_to_csv(self, filepath):\n",
    "        self.df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw dataset\n",
    "\n",
    "dataset: https://purl.stanford.edu/bq914sc3730\n",
    "\n",
    "paper using this dataset: https://www.sciencedirect.com/science/article/pii/S0031320322002382?casa_token=uoQfTQnrCZoAAAAA:P6d4nfYk9FUyy6gTpGNJM6eAINAjuNaxnzwf-5RNNLu9ubl7B6WY3YDjMEaz38SKfIgxalScAfs#sec0012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stanford_trts(args, sub):\n",
    "    SCALE = 0.1\n",
    "    #for sub in range(1,11):\n",
    "    mat = scipy.io.loadmat(os.path.join(args['data_dir'],f'S{sub}.mat'))  \n",
    "    X = mat['X_3D'] # (electrodes, time, trials)\n",
    "    X = np.transpose(X, (2, 1, 0)) # transform into (trials, time, electrodes)\n",
    "    X *= SCALE\n",
    "    Y = mat['categoryLabels'][0] # [1 2 3 4 5 6] different categories   \n",
    "    X_new = X #np.concatenate([X_new, X]) if X_new.size else X\n",
    "    Y_new = Y #np.concatenate([Y_new, Y]) if Y_new.size else Y\n",
    "\n",
    "    mean, std = X_new.mean(), X_new.std()\n",
    "    print(\"Input data: mean =\", mean, \", std =\", std) # used for input data Normalization\n",
    "    print(\"Stanford Dataset shape:\", X_new.shape, \", min:\", np.min(X_new), \", max:\", np.max(X_new))\n",
    "    Y_new = Y_new - 1 # [0 1 2 3 4 5] category indexes \n",
    "    print(\"category indexes:\", np.unique(Y_new))\n",
    "    \n",
    "    # Random split. \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(np.float32(X_new), Y_new.astype(int), test_size=0.1, random_state=1)   \n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "    # Convert data to DataLoader\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Deep neural network based on a basic Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetTraST(nn.Module):\n",
    "    def __init__(self, args): \n",
    "        super(NetTraST, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(args['vocab_size'])\n",
    "        p = args['kernel_size'] // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels=args['vocab_size'], out_channels=args['kernel_num'], kernel_size=args['kernel_size'], stride=1, padding=p)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=args['embed_dim'], out_channels=args['kernel_num'], kernel_size=args['kernel_size'], stride=1, padding=p)\n",
    "        self.upsamp = nn.Upsample((args['embed_dim']))\n",
    "        \n",
    "        self.rrelu = nn.RReLU(0.1, 0.3)\n",
    "        nl=3 #args['num_layers']//2\n",
    "        self.spatial_tra = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args['embed_dim'],\n",
    "                nhead=args['nhead'],\n",
    "                dim_feedforward=args['dim_feedforward'],\n",
    "            ),\n",
    "            num_layers=nl,\n",
    "        )\n",
    "        self.temporal_tra = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args['vocab_size'],\n",
    "                nhead=args['nhead'],\n",
    "                dim_feedforward=args['dim_feedforward'],\n",
    "            ),\n",
    "            num_layers=nl,\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args['kernel_num'],\n",
    "                nhead=args['nhead'],\n",
    "                dim_feedforward=args['dim_feedforward'],\n",
    "            ),\n",
    "            num_layers=args['num_layers'],\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.batch_norm3 = nn.BatchNorm1d(args['kernel_num'])\n",
    "        self.fl = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(args['kernel_num']*args['embed_dim'], args['kernel_num'])\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.fc2 = nn.Linear(args['kernel_num'], args['class_num'])\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x1 = self.conv1(x) \n",
    "        x1 = self.spatial_tra(x1)\n",
    "        \n",
    "        x2 = x.permute(0, 2, 1)\n",
    "        x2 = self.conv2(x2) \n",
    "        x2 = self.temporal_tra(x2)\n",
    "        x2 = self.upsamp(x2)\n",
    "\n",
    "        x = x1+x2 \n",
    "        #x = torch.cat((x1, x2), 1)\n",
    "        \n",
    "        # Reshape the input for the Transformer layer\n",
    "        #x = x.permute(2, 0, 1)  # Change the shape to (sequence_length, batch_size, input_size)\n",
    "        #x = self.transformer(x)\n",
    "        # Reshape the output back to the original shape\n",
    "        #x = x.permute(1, 2, 0)  # Change the shape to (batch_size, input_size, sequence_length)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.fl(x)\n",
    "        x = self.rrelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_raw(args, model, test_loader, criterion):\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0\n",
    "        test_corrects = torch.tensor(0, device=args['device'])\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            corrects = (torch.max(outputs, 1)[1].view(labels.size()).data == labels.data).sum()\n",
    "            test_corrects += corrects\n",
    "            tot_loss += loss\n",
    "\n",
    "        ts_acc = 100.0 * test_corrects/len(test_loader.dataset)\n",
    "    return ts_acc.cpu().item(), tot_loss\n",
    "\n",
    "\n",
    "def train_raw(args, model, train_loader, optimizer, criterion, test_loader, metrics, fold, scheduler=None): \n",
    "    best_acc = 0\n",
    "    patience_counter = 0\n",
    "    steps = 0\n",
    "    loop_obj = tqdm(range(args['epochs']))\n",
    "    loop_obj.set_postfix_str(f\"Best val. acc.: {best_acc:.4f}\")  # Adds text after progressbar\n",
    "    for epoch in loop_obj:\n",
    "        loop_obj.set_description(f\"Subj.: {fold}, Training epoch: {epoch+1}\")  # Adds text before progessbar\n",
    "        train_corrects = torch.tensor(0, device=args['device'])\n",
    "        tot_loss = 0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(args['device'])\n",
    "\n",
    "            labels = labels.to(args['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            corrects = (torch.max(outputs, 1)[1].view(labels.size()).data == labels.data).sum()\n",
    "            train_corrects += corrects\n",
    "            tot_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler: scheduler.step()\n",
    "\n",
    "        tr_acc = 100.0 * train_corrects/len(train_loader.dataset)\n",
    "        # Validation\n",
    "        dev_acc, test_loss = evaluation_raw(args, model, test_loader, criterion)\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            best_acc = dev_acc\n",
    "            patience_counter = 0\n",
    "            loop_obj.set_postfix_str(f\"Best val. acc.: {best_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        lr=optimizer.param_groups[0][\"lr\"]\n",
    "        metrics.add_row([epoch+1, lr, tot_loss.cpu().item(), tr_acc.cpu().item(), test_loss.cpu().item(), dev_acc, best_acc])\n",
    "        metrics.save_to_csv(os.path.join(args['save_dir'], \"metrics_classifciation.csv\"))\n",
    "\n",
    "        if patience_counter >= args['early_stopping_patience']:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run an experiment\n",
    "\n",
    "- change the parameter '*data_dir*' in the *args* dictionary \n",
    "- remenber to change also the '*save_dir*' parameter\n",
    "\n",
    "We evaluated our models for only one classification task, namely, \n",
    "- 6-class category-level classification, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_args():\n",
    "    args = {\n",
    "        'class_num': 6,\n",
    "        'dropout': 0.1 ,\n",
    "        'nhead': 4 , \n",
    "        'dim_feedforward': 128 ,\n",
    "        'num_layers': 14, \n",
    "        'embed_dim': 124,\n",
    "        'vocab_size': 32,\n",
    "        'kernel_num': 56, \n",
    "        'kernel_size': 3, \n",
    "        'batch_size': 64, # use low batch size\n",
    "        'epochs': 1000 ,\n",
    "        'early_stopping_patience': 300,\n",
    "        'lr': 0.9 ,\n",
    "        'log_interval': 1,\n",
    "        'device': 'cuda:1', # cuda, cpu\n",
    "        'data_dir': '/home/jovyan/nfs/igallo/datasets/EEG/stanford/', # S1.mat, ..., S10.mat\n",
    "        'save_dir': 'experiments/transformer/stanford/', # \n",
    "        'save_best': True,\n",
    "        'verbose': True,\n",
    "        'test_interval': 100,\n",
    "        'save_interval': 500,\n",
    "        'sampling_rate': 62.5,\n",
    "        'k_folds': 10,\n",
    "    }\n",
    "    # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['gpus']\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'num_layers': 14, for the final transformer\n",
    "- 'num_layers': 3, for the first two transformers\n",
    "- 'kernel_num': 56, \n",
    "- 'epochs': 1000 ,\n",
    "- 'early_stopping_patience': 300,\n",
    "\n",
    "### Ablation 5\n",
    "- NO Transformer T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run():\n",
    "    args = get_default_args()\n",
    "    create_save_dir(args)\n",
    "    with open(os.path.join(args['save_dir'], \"config.yaml\"), \"w\") as f:\n",
    "        yaml.dump(\n",
    "            args, stream=f, default_flow_style=False, sort_keys=False\n",
    "        )\n",
    "\n",
    "    # For all the results\n",
    "    metrics = Metrics([\"epoch\", \"lr\", \"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\", \"best_test_acc\"])\n",
    "    results = {}\n",
    "    for sub in range(1,11):\n",
    "        model = NetTraST(args)\n",
    "        model = model.to(args['device'])\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters()) #, lr=args['lr'])\n",
    "\n",
    "        train_loader, test_loader = read_stanford_trts(args, sub) \n",
    "        print(\"Training size:\", len(train_loader.dataset))\n",
    "        print(\"Test size:\", len(test_loader.dataset))\n",
    "\n",
    "        best_acc = train_raw(args, model, train_loader, optimizer, criterion, test_loader, metrics, sub) \n",
    "        #torch.save(model, os.path.join(args['save_dir'], f\"{model.__class__.__name__}_model_last.pt\"))\n",
    "        results[sub] = best_acc\n",
    "        accs = np.array(list(results.values()))\n",
    "        print(f'acc: mean={np.mean(accs):.4f}%, std={np.std(accs):.4f}%')\n",
    "\n",
    "    # Print subject results\n",
    "    str = f\"RESULTS FOR 10 SUBJECTS\\n\"\n",
    "    str += '--------------------------------\\n'\n",
    "    for key, value in results.items():\n",
    "        str += f'Subject {key}: {value:.4f} %\\n'\n",
    "    accs = np.array(list(results.values()))\n",
    "    str += f'mean: {np.mean(accs):.4f}%, std: {np.std(accs):.4f}%\\n'\n",
    "    print(str)\n",
    "    with open(os.path.join(args['save_dir'], \"mean_results.txt\"), \"w\") as f:\n",
    "        f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in: experiments/transformer/stanford/2023-12-11_09-05-26\n",
      "Input data: mean = 1.7321293941255597e-20 , std = 0.015957179516901867\n",
      "Stanford Dataset shape: (5188, 32, 124) , min: -1.2147637743651565 , max: 3.0813462468876276\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4669\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 1, Training epoch: 309:  31%|███       | 308/1000 [10:13<22:59,  1.99s/it, Best val. acc.: 47.2062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "acc: mean=47.2062%, std=0.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: mean = 6.64409829780833e-23 , std = 0.009863266299745401\n",
      "Stanford Dataset shape: (5185, 32, 124) , min: -1.0336102391247837 , max: 1.4079203544836336\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4666\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 2, Training epoch: 302:  30%|███       | 301/1000 [14:29<33:39,  2.89s/it, Best val. acc.: 43.7380]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "acc: mean=45.4721%, std=1.7341%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: mean = -1.968061026495542e-21 , std = 0.02183792022836993\n",
      "Stanford Dataset shape: (5186, 32, 124) , min: -4.674710928833426 , max: 4.793877824490399\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4667\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 3, Training epoch: 740:  74%|███████▍  | 739/1000 [25:22<08:57,  2.06s/it, Best val. acc.: 52.4085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "acc: mean=47.7842%, std=3.5632%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: mean = -2.21539637523785e-21 , std = 0.01804324489169541\n",
      "Stanford Dataset shape: (5186, 32, 124) , min: -8.706135948108061 , max: 9.014969876595016\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4667\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 4, Training epoch: 304:  30%|███       | 303/1000 [07:58<18:21,  1.58s/it, Best val. acc.: 46.6281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "acc: mean=47.4952%, std=3.1262%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: mean = 2.7507241480672456e-21 , std = 0.01296664363084747\n",
      "Stanford Dataset shape: (5185, 32, 124) , min: -1.0386944604699646 , max: 1.3983891328368285\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4666\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 5, Training epoch: 489:  49%|████▉     | 488/1000 [12:18<12:54,  1.51s/it, Best val. acc.: 58.7669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n",
      "acc: mean=49.7495%, std=5.3053%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: mean = 1.2162762139243417e-21 , std = 0.010993911972572074\n",
      "Stanford Dataset shape: (5186, 32, 124) , min: -2.207850458413234 , max: 2.1549545400402357\n",
      "category indexes: [0 1 2 3 4 5]\n",
      "Training size: 4667\n",
      "Test size: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subj.: 6, Training epoch: 141:  14%|█▍        | 140/1000 [03:26<20:37,  1.44s/it, Best val. acc.: 62.8131]"
     ]
    }
   ],
   "source": [
    "single_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_clip_venv",
   "language": "python",
   "name": "openai_clip_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
